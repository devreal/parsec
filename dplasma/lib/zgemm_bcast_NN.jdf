extern "C" %{
/*
 * Copyright (c) 2017-2017 The University of Tennessee and The University
 *                         of Tennessee Research Foundation. All rights
 *                         reserved.
 *
 * @precisions normal z -> s d c
 * $COPYRIGHT
 *
 */

#include <math.h>

#include "dplasma/lib/dplasmajdf.h"
#include "dplasma/lib/dplasmaaux.h"
#include "dplasma_bcast.h"
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"
#include "parsec/data_dist/matrix/irregular_tiled_matrix.h"
//#include "summa_z.h"
#include "flops.h"

#if defined(PARSEC_HAVE_CUDA)
#include <cublas.h>
#endif  /* defined(PARSEC_HAVE_CUDA) */

#if defined(PARSEC_HAVE_RECURSIVE)
#include "parsec/data_dist/matrix/irregular_subtile.h"
#endif

static int pipeline_step(int nb_cuda_devices) {
    if(nb_cuda_devices == 0)
        return 1;
    return nb_cuda_devices;
}

static int use_ctl(int ctlmode) {
    return ctlmode == -1 || ctlmode == -2;
}

static int use_prefetch(int ctlmode) {
    return ctlmode == -2;
}
%}

/* Keep this first, as in all jdf in this directory, to
 * enable switching between SUMMA implementations.
 * This is the JDF-like CRTP ;)
 */
summa_type [type = int]

transA [ type = int ]
transB [ type = int ]
alpha  [ type = parsec_complex64_t ]

descA    [ type = "const irregular_tiled_matrix_desc_t *" ]
descB    [ type = "const irregular_tiled_matrix_desc_t *" ]
descC    [ type = "irregular_tiled_matrix_desc_t *"       ]
TrivDist [ type = "parsec_data_collection_t *"            ]

plan              [ type = "gemm_plan_t *" ]
nb_cuda_devices   [ type = "int"           ]
cuda_device_index [ type = "int *"         ]

ctlmode [type = "int" hidden=on default="-1"]

/**************************************************
 *                       READ_A                   *
 **************************************************/
READ_A(m, k)
  m = 0 .. descA->mt-1
  k = 0 .. descA->nt-1
  mb = %{ return descA->Mtiling[m]; %}
  nb = %{ return descA->Ntiling[k]; %}

:descA(m, k)

READ A <- descA(m, k)    [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descA, m, k); %} ]                  
       -> A GEMM(m, 0 .. descC->nt-1, k) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descA, m, k); %} ]

BODY
 /* Nothing */
END

/**************************************************
 *                       READ_B                   *
 **************************************************/
READ_B(k, n)
  k = 0 .. descB->mt-1
  n = 0 .. descB->nt-1
  mb = %{ return descB->Mtiling[k]; %}
  nb = %{ return descB->Ntiling[n]; %}

: descB(k, n)

READ B <- descB(k, n)   [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descB, k, n); %} ]
       -> B GEMM(0 .. descC->mt-1, n, k) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descB, k, n); %} ]

BODY
  /* Nothing */
END

/**************************************************
 *                       MAKE_C                   *
 **************************************************/

/* MAKE_C(m, n, r) represents the beginning of the chain of local
 * contributions for the C(m, n) on rank r */
MAKE_C(m, n, r)
  m = 0 .. descC->mt-1
  n = 0 .. descC->nt-1
  r = 0 .. %{ return gemm_plan_highest_rank(plan, m, n); %}
  kfirst = %{ return gemm_plan_kfirst(plan, m, n, r);    %}

: TrivDist(r)

WRITE C <- NEW                                  [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]
        -> kfirst == -1 ? Cl ACC_C(m, n, r)     
                        : C  GEMM(m, n, kfirst) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ] 

CTL Z <- %{ return use_ctl(ctlmode); %} ? Z LOCAL_BARRIER(n, r)

BODY
  memset(C, 0, sizeof(parsec_complex64_t) * get_tile_count(descC, m, n));
  if( nb_cuda_devices > 0 ) {
      /* Advise that all updates on this new C tile should go on device for that column */
      parsec_advise_data_on_device( _f_C->original,
      				    n % nb_cuda_devices,
				    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE);
  }
END

/**************************************************
 *                       ACC_C                    *
 **************************************************/
 
 /* ACC_C(m, n, r) represents the end of the chain of local
 * contributions for the C(m, n) on rank r */
ACC_C(m, n, r)
  m = 0 .. descC->mt-1
  n = 0 .. descC->nt-1
  r = 0 .. %{ return gemm_plan_highest_rank(plan, m, n); %}
  klast  = %{ return gemm_plan_klast(plan, m, n, r); %}
  rfirst = %{ return gemm_plan_rank_first(plan, m, n); %}
  rlast  = %{ return gemm_plan_rank_last(plan, m, n); %}
  rnext  = %{ return gemm_plan_rank_next(plan, m, n, r); %}
  rprev  = %{ return gemm_plan_rank_prev(plan, m, n, r); %}

: TrivDist(r)

/* Cl: local contribution for C(m, n). This task adds the remote
       contribution (if any) and sends it to the next accumulate task
       (if any), or it adds the computed sum to the user memory
   Cm: if the last node, pointer to the user data, otherwise NULL
   Cr: remote contribution for C(m, n) */

RW Cl <- klast == -1 ? C MAKE_C(m, n, r)       
                     : C GEMM(m, n, klast)     [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]
      -> (r != rlast) ? Cr ACC_C(m, n, rnext)  [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]

READ Cr <- (r == rfirst) ? NULL                  
                         : Cl ACC_C(m, n, rprev) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]

RW Cm <- (r == rlast) ? descC(m, n)            
                      : NULL                   [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]
      -> (r == rlast) ? descC(m, n)            [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]

CTL Z -> %{ return use_ctl(ctlmode) && (n + pipeline_step(nb_cuda_devices) < descC->nt); %} ? Z LOCAL_BARRIER(%{ return n + pipeline_step(nb_cuda_devices); %}, r)

BODY
{
    parsec_complex64_t za = (parsec_complex64_t)1.0;
    int cCmb = descC->Mtiling[m];
    int cCnb = descC->Ntiling[n];

    if( r != rfirst ) {
    	CORE_zgeadd(PlasmaNoTrans, cCmb, cCnb, za, Cr, cCmb, za, Cl, cCmb);
    }
    if( r == rlast ) {
        CORE_zgeadd(PlasmaNoTrans, cCmb, cCnb, za, Cl, cCmb, za, Cm, cCmb);
    }    
}
END

/**************************************************
 *                  LOCAL_BARRIER                 *
 **************************************************/

/* If use_ctl(ctlmode), LOCAL_BARRIER(n, r) is a barrier that prevents any
 * chain of GEMM at column n on rank r to be scheduled before it has triggered.
 * It is triggered by receiving a control gather from all local ACC_C for column n-pipeline_step(nb_cuda_devices).
 * If use_prefetch(ctlmode), LOCAL_BARRIER(n, r) will also prefetch all local
 * contributions of B(m, n + pipeline_step(nb_cuda_devices)) on the gpu (n+pipeline_step(nb_cuda_devices)) % #gpus */
LOCAL_BARRIER(n, r)
  n = 0 .. descC->nt-1
  r = 0 .. %{ return descC->super.nodes - 1; %}

: TrivDist(r)

CTL Z <- %{ return use_ctl(ctlmode) && (n >= pipeline_step(nb_cuda_devices)); %} ? Z ACC_C(0 .. descC->mt-1, %{return n-pipeline_step(nb_cuda_devices);%}, r)
      -> %{ return use_ctl(ctlmode); %}                                          ? Z MAKE_C(0 .. descC->mt-1, n, r)

BODY
{
    if( use_prefetch(ctlmode) && nb_cuda_devices > 0 ) {
        int i;
        /* If n < pipeline_step(nb_cuda_devices), then we want to prefetch the B for the current column before
         * we start prefetching the B for the future one */
        if( n < pipeline_step(nb_cuda_devices) && n < descB->nt ) {
            for(i = 0; i < descB->mt; i++) {
                if( (int)descB->super.rank_of((parsec_data_collection_t *)descB, i, n) == r ) {
                    parsec_advise_data_on_device( descB->super.data_of((parsec_data_collection_t *)descB, i, n),
                                                  cuda_device_index[ n % nb_cuda_devices ], PARSEC_DEV_DATA_ADVICE_PREFETCH);
                }
            }
        }
        /* In the 'normal' case, prefetch all local B on column n + pipeline_step(nb_cuda_devices) in advance onto the GPU */
        if( n + pipeline_step(nb_cuda_devices) < descB->nt ) {
            for(i = 0; i < descB->mt; i++) {
                if( (int)descB->super.rank_of((parsec_data_collection_t *)descB, i, n+pipeline_step(nb_cuda_devices)) == r ) {
                    parsec_advise_data_on_device( descB->super.data_of((parsec_data_collection_t *)descB, i, n + pipeline_step(nb_cuda_devices)),
                                                  cuda_device_index[ (n+pipeline_step(nb_cuda_devices)) % nb_cuda_devices ], PARSEC_DEV_DATA_ADVICE_PREFETCH);
                }
            }
        }
    }
}
END

/**************************************************
 *                       GEMM                     *
 **************************************************/

GEMM(m, n, k)
  m = 0 .. descC->mt-1
  n = 0 .. descC->nt-1
  k = 0 .. descB->mt-1
  r = %{ return descB->super.rank_of((parsec_data_collection_t *)descB, k, n); %}
  kprev = %{ return gemm_plan_kprev(plan, m, n, r, k); %}
  knext = %{ return gemm_plan_knext(plan, m, n, r, k); %}

: TrivDist(r)

READ A <- A READ_A(m ,k) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descA, m, k); %} ]
READ B <- B READ_B(k, n) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descB, k, n); %} ]
RW C <- kprev == -1
          ? C MAKE_C(m, n, r)    
          : C GEMM(m, n, kprev ) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]
     -> knext == -1
          ? Cl ACC_C(m, n, r)    
          : C  GEMM(m, n, knext) [ type= DEFAULT layout=parsec_datatype_double_complex_t  count= %{ return get_tile_count(descC, m, n); %} ]

BODY [type=CUDA
      dyld=cublasZgemm dyldtype=cublas_zgemm_t
      weight=(descA->nt-k)]
{
#if defined(PRECISION_z) || defined(PRECISION_c)
    cuDoubleComplex lalpha = make_cuDoubleComplex(creal(alpha), cimag(alpha));
    cuDoubleComplex lbeta  = (k == 0) ? make_cuDoubleComplex(0.0, 0.0) : make_cuDoubleComplex(1.0, 0.0);
#else
    double lalpha = alpha;
    double lbeta  = (k==0) ? 0.0:1.0;
#endif
    int cAmb = descA->Mtiling[m];
    int cAnb = descA->Ntiling[k];
    int cBmb = descB->Mtiling[k];
    int cBnb = descB->Ntiling[n];
    int cCmb = cAmb;
    int cCnb = cBnb;

    int tempmm = cCmb;
    int tempnn = cCnb;
    int tempkk = cAnb;
    int ldam = cAmb;
    int ldbk = cBmb;
    int ldcm = cCmb;

#if defined(PARSEC_DEBUG_NOISIER)
    /* fprintf(stdout, */
    /*         "CUDA: gemm( %d, %d, %d ) > A(%d,%d) * B(%d,%d) C(%d,%d)\n", */
    /*         m, n, k, cAmb, cAnb, cBmb, cBnb, cCmb, cCnb); */
#endif

    cublasStatus_t status;
    cublasSetKernelStream( parsec_body.stream );
    parsec_body.dyld_fn( lapack_const(transA), lapack_const(transB),
             tempmm, tempnn, tempkk,
             lalpha, (cuDoubleComplex*)A, ldam,
                     (cuDoubleComplex*)B, ldbk,
             lbeta,  (cuDoubleComplex*)C, ldcm );
    status = cublasGetError();
    PARSEC_CUDA_CHECK_ERROR( "cublasZgemm ", status,
                            {return -1;} );

    /* Quick and dirty emulation of the next GEMM */
    if( knext != -1 ) {
        __parsec_zgemm_bcast_NN_GEMM_task_t next_gemm;
        memcpy(&next_gemm, this_task, sizeof(__parsec_zgemm_bcast_NN_GEMM_task_t));
        next_gemm.locals.k.value = knext;
        assert( PARSEC_DEV_CUDA == next_gemm.task_class->incarnations[this_task->chore_id].type );
        if(NULL != next_gemm.task_class->incarnations[this_task->chore_id].evaluate) {
            if( next_gemm.task_class->incarnations[this_task->chore_id].evaluate((parsec_task_t*)&next_gemm) ==
                PARSEC_HOOK_RETURN_NEXT ) {
                    /* The next GEMM wants to run on the CPUs... */
                    gpu_task->pushout |= (1 << 0);
            }
        }
    }
    
}
END

BODY
{
    parsec_complex64_t lbeta = (k==0) ? (parsec_complex64_t)0.0 : (parsec_complex64_t)1.0;

    int cAmb = descA->Mtiling[m];
    int cAnb = descA->Ntiling[k];
    int cBmb = descB->Mtiling[k];
    int cBnb = descB->Ntiling[n];
    int cCmb = cAmb;
    int cCnb = cBnb;

    int tempmm = cCmb;
    int tempnn = cCnb;
    int tempkk = cAnb;
    int ldam = cAmb;
    int ldbk = cBmb;
    int ldcm = cCmb;

#if !defined(PARSEC_DRY_RUN)
    CORE_zgemm(transA, transB,
               tempmm, tempnn, tempkk,
               alpha, A /*A(m, k)*/, ldam,
                      B /*B(k, n)*/, ldbk,
               lbeta, C /*C(m, n)*/, ldcm);
#endif  /* !defined(PARSEC_DRY_RUN) */

    printlog("gemm( %d, %d, %d )\n"
             "    ( %s, %s, %d, %d, %d, %f, A(%d,%d), %d, B(%d,%d), %d, %f, C(%d,%d), %d)\n",
             m, n, k,
             plasma_const( transA ), plasma_const( transB ),
             tempmm, tempnn, tempkk,
             creal(alpha), m, k, ldam,
                           k, n, ldbk,
             creal(lbeta), m, n, ldcm );
}
END
